{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Model Notebook\n",
        "This notebook organizes the finetuning process for a model using LoRA adjustments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LoRA Configuration\n",
        "Define configuration parameters for the LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                                    # LoRA rank\n",
        "    lora_alpha=16,                          # scaling factor\n",
        "    lora_dropout=0.01,                      # dropout probability\n",
        "    task_type=TaskType.CAUSAL_LM,           # specify the task\n",
        "    target_modules = \"all-linear\"           # layers to inject trainable adapters \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer and Model Initialization\n",
        "Load the tokenizer and model, and apply the LoRA modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import get_peft_model\n",
        "\n",
        "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization and Data Preparation\n",
        "Define a function to tokenize each message and prepepare dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def tokenize_message(json_messages):\n",
        "    messages = json_messages[\"messages\"]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "chatml_path = \"../data/yugioh_rulebook_chatml.jsonl\"\n",
        "train_dataset = load_dataset(\"json\", data_files=chatml_path, split=\"train\")\n",
        "train_dataset = train_dataset.map(tokenize_message, batched=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trainer Initialization and Training\n",
        "Initialize the Trainer with training arguments and datasets, then start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=3e-4\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Saving\n",
        "After training, save the finetuned model to the specified adapter path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adapter_path = \"../adapters/yugioh_phi_4\"\n",
        "model.save_pretrained(adapter_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
