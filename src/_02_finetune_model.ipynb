{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Model Notebook\n",
        "This notebook organizes the finetuning process for a model using LoRA adjustments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration and Setup\n",
        "Define paths and configuration parameters for the model, LoRA setup, and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,   # specify the task\n",
        "    r=8,                            # LoRA rank\n",
        "    lora_alpha=16,                  # scaling factor\n",
        "    lora_dropout=0.01,              # dropout probability\n",
        "    bias=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer and Model Initialization\n",
        "Load the tokenizer and model, and apply the LoRA modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/llama-3.2-1B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization\n",
        "Define a function to tokenize each message using the model's chat template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_message(json_messages):\n",
        "    messages = json_messages[\"messages\"]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "Tokenize the dataset and split it into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chatml_path = \"./yugioh_rulebook_dataset_chatml.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files=chatml_path)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_message, batched=False)\n",
        "\n",
        "train_test_datasets = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trainer Initialization and Training\n",
        "Initialize the Trainer with training arguments and datasets, then start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=3e-4\n",
        "    ),\n",
        "    train_dataset=train_test_datasets[\"train\"],\n",
        "    eval_dataset=train_test_datasets[\"test\"]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Saving\n",
        "After training, save the finetuned model to the specified adapter path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adapter_path = \"./adapters/llama_3_2\"\n",
        "model.save_pretrained(adapter_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
